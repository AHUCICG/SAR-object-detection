CRTransSar: A visual transformer based on contextual joint representation learning for SAR target detection
SAR image target detection has been widely used in military, civilian and other fields. The existing detection methods have low accuracy due to the limitations presented by the strong scattering of SAR image targets, unclear edge contour information, multiscaling, strong sparseness, background interference and other characteristics. As a result, for SAR target detection tasks, this paper combines the global contextual information perception of transformers and the local feature representation capabilities of CNNs to innovatively propose a visual transformer framework based on contextual joint representation learning, referred to as CRTransSar. First, this article introduces the latest Swin Transformer as the basic architecture. Then, it introduces the CNN's local information capture and designs a backbone, CRbackbone, based on contextual joint representation learning to extract richer contextual feature information, while strengthening SAR target feature attributes. Furthermore, a new cross-resolution attention enhancement neck, CAENeck, is designed to enhance the characterizability of multiscale SAR targets. The mAP of our method on the SSDD dataset attains 97.0%, reaching state-of-the-art. In addition, based on the HISEA-1 commercial SAR satellite that our research group has participated in the development and has been launched into orbit, we released a larger-scale SAR multiclass target detection dataset called SMCDD, which verifies the effectiveness of our method.
